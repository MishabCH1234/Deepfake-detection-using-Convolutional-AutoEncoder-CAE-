{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":26749,"status":"ok","timestamp":1748338806664,"user":{"displayName":"Mishab ch","userId":"00818002393088854647"},"user_tz":-330},"id":"ZUXm5DZee71h"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset, DataLoader\n","import matplotlib.pyplot as plt\n","import os\n","import tqdm\n","from PIL import Image\n","import shutil\n","import csv\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19320,"status":"ok","timestamp":1730103996817,"user":{"displayName":"Mishab ch","userId":"00818002393088854647"},"user_tz":-330},"id":"hsy6uYXhg5PG","outputId":"8b083333-be33-4cd2-b7f3-dd35de08565d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uHMjXPjqe71j"},"outputs":[],"source":["def precision(y_pred, y_true):\n","    \"\"\"\n","    Calculates precision for binary classification.\n","\n","    Args:\n","    - y_pred (Tensor): Predicted labels (0 or 1).\n","    - y_true (Tensor): True labels (0 or 1).\n","\n","    Returns:\n","    - precision (float): Precision score.\n","    \"\"\"\n","    true_positives = torch.logical_and(y_pred == 1, y_true == 1).sum().item()\n","    predicted_positives = (y_pred == 1).sum().item()\n","    precision = true_positives / (predicted_positives + 1e-20)  # Adding epsilon to avoid division by zero\n","    return precision"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jyT6n8-1e71j"},"outputs":[],"source":["def log_to_csv(filename, data):\n","    file_exists = os.path.isfile(filename)\n","    with open(filename, mode='a') as csv_file:\n","        reader = csv.reader(csv_file)\n","        if reader.readrows()>0:\n","            writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n","        if not file_exists:\n","            writer.writerow(['Epoch', 'Loss', 'Accuracy', 'Precision'])\n","        writer.writerow(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WQmIEdTGm9hY"},"outputs":[],"source":["from torchvision.transforms import Compose, Resize, ToTensor\n","class CustomDataset(Dataset):\n","    def __init__(self, data_folder, transform=None):\n","        self.data_folder = data_folder\n","        self.transform = transform\n","\n","        # Get a list of all image files in the folder\n","\n","        dirs=[os.path.join(data_folder,f) for f in os.listdir(data_folder)]\n","        #at=[os.path.join(data_folder,f) for f in os.listdir(data_folder) if f.endswith(('.jpg', '.jpeg', '.png', '.gif'))]\n","        self.image_files=[]\n","        ant=[dirs[0], dirs[1]]\n","        lab=torch.tensor([0, 1])\n","        for j,i in enumerate(dirs):\n","            self.image_files.extend([(os.path.join(ant[j],f),lab[j]) for f in os.listdir(i)[:1000]])\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.image_files[idx][0]\n","        image = Image.open(img_name).convert('RGB')\n","        if self.transform:\n","            image = self.transform(image)\n","        return image[0].reshape(1,256,256), self.image_files[idx][1]\n","\n","def data_define(data_folder=False, tensor=False):\n","\n","    transform = Compose([\n","        ResizeToSquare(256),\n","        ToTensor()\n","    ])\n","\n","    if data_folder:\n","        real=[os.path.join('/content/drive/MyDrive/deep fake detection/Dataset/Train/Celeb-real-frame', i) for i in os.listdir('/content/drive/MyDrive/deep fake detection/Dataset/Train/Celeb-real-frame')]\n","        fake=[os.path.join('/content/drive/MyDrive/deep fake detection/Dataset/Train/Celeb-fake-frame', i) for i in os.listdir('/content/drive/MyDrive/deep fake detection/Dataset/Train/Celeb-fake-frame')]\n","        for j,dir in enumerate([real, fake]):\n","            for i in dir:\n","                if j==0:\n","                    shutil.copy(i,'/content/drive/MyDrive/Test2/Real')\n","                else:\n","                    shutil.copy(i,'/content/drive/MyDrive/Test2/Fake')\n","        data_folder = '/content/drive/MyDrive/Test2'\n","        custom_dataset = CustomDataset(data_folder, transform=transform)\n","        dataloader = DataLoader(custom_dataset, batch_size=64, shuffle=True)\n","        return custom_dataset, dataloader\n","\n","    elif tensor:\n","        image= tensor\n","        if transform:\n","            image = transform(image)\n","        return image[0].reshape(1,256,256)\n","\n","class ResizeToSquare(object):\n","    def __init__(self, size):\n","        self.size = size\n","\n","    def __call__(self, img):\n","        width, height = img.size\n","        aspect_ratio = width / height\n","        if aspect_ratio > 1:\n","            new_width = self.size\n","            new_height = int(self.size / aspect_ratio)\n","        else:\n","            new_height = self.size\n","            new_width = int(self.size * aspect_ratio)\n","        img = img.resize((new_width, new_height))\n","        canvas = Image.new('RGB', (self.size, self.size), (0, 0, 0))\n","        h_offset = (self.size - new_width) // 2\n","        v_offset = (self.size - new_height) // 2\n","        canvas.paste(img, (h_offset, v_offset))\n","        return canvas"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"2MgUts15e71l","scrolled":false},"outputs":[],"source":["ds, trainloader=data_define('/content/')"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"t3t5ZdsrNwaf","executionInfo":{"status":"ok","timestamp":1748338946742,"user_tz":-330,"elapsed":3479,"user":{"displayName":"Mishab ch","userId":"00818002393088854647"}}},"outputs":[],"source":["class Classifier(nn.Module):\n","    def __init__(self):\n","        super(Classifier, self).__init__()\n","\n","        # Encoder\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(1, 128, kernel_size=5, stride=2, padding=2),\n","            nn.LeakyReLU(0.1, inplace=True),\n","            nn.Conv2d(128, 256, kernel_size=5, stride=2, padding=2),\n","            nn.LeakyReLU(0.1, inplace=True),\n","            nn.Conv2d(256, 512, kernel_size=5, stride=2, padding=2),\n","            nn.LeakyReLU(0.1, inplace=True),\n","            nn.Conv2d(512, 1024, kernel_size=5, stride=2, padding=2),\n","            nn.LeakyReLU(0.1, inplace=True),\n","            nn.Flatten(),\n","            # Correct the input features for the first linear layer based on the 128x128 input size\n","            nn.Linear(1024 * 8 * 8, 1024),\n","            nn.LeakyReLU(0.1, inplace=True),\n","            nn.Linear(1024, 2)  # Two output classes: real or fake\n","        )\n","\n","        # Decoder - Note: The decoder dimensions are also likely incorrect if designed for a 16x16 spatial size after convolution.\n","        # If the decoder is intended to reconstruct the original 256x256 image, its architecture and the first linear layer\n","        # might need to be adjusted as well based on the output of the encoder's classification layer (which has 2 features).\n","        # However, the current error is in the encoder's forward pass during `summary`.\n","        self.decoder_fc = nn.Sequential(\n","            nn.Linear(2, 1024), # This takes the 2 output features from the encoder\n","            nn.LeakyReLU(0.1, inplace=True),\n","            # This linear layer should output a flattened tensor that can be reshaped to (1024, H, W) for the first ConvTranspose2d\n","            # If the ConvTranspose2d expects (1024, 16, 16), the output here should be 1024 * 16 * 16.\n","            # If the encoder's architecture was designed for a different input size leading to 16x16 spatial dimensions,\n","            # the decoder's linear layer size might be correct in that context, but the encoder's linear layer wasn't.\n","            # For a 128x128 input resulting in 8x8 spatial dimension before flatten, the encoder output is 2 features.\n","            # The decoder starts from these 2 features. If you want to reconstruct a 256x256 image, the decoder needs to upsample.\n","            # The intermediate size (1024, 16, 16) in the decoder suggests an attempt to upsample back to something related to 16x16.\n","            # If the goal is reconstruction, the decoder's input to ConvTranspose2d (after unflatten) should correspond to the\n","            # size before flatten in the encoder's mirrored architecture. The encoder ends with 1024 channels and 8x8 spatial.\n","            # So the decoder's first ConvTranspose2d would ideally take 1024 channels and start from 8x8 spatial.\n","            # The current decoder architecture is likely mismatched for the 128x128 input and the encoder's structure.\n","            # For now, let's fix the encoder. If reconstruction is needed, the decoder will require significant review.\n","            nn.Linear(1024, 1024 * 16 * 16), # This is likely incorrect for reconstructing from 2 features to 256x256.\n","            nn.LeakyReLU(0.1, inplace=True)\n","        )\n","\n","        self.decoder_conv = nn.Sequential(\n","            nn.Unflatten(1, (1024, 16, 16)), # This expects the output of decoder_fc to be reshapeable to (1024, 16, 16)\n","            nn.ConvTranspose2d(1024, 512, kernel_size=5, stride=2, padding=2, output_padding=1),\n","            nn.LeakyReLU(0.1, inplace=True),\n","            nn.ConvTranspose2d(512, 256, kernel_size=5, stride=2, padding=2, output_padding=1),\n","            nn.LeakyReLU(0.1, inplace=True),\n","            nn.ConvTranspose2d(256, 128, kernel_size=5, stride=2, padding=2, output_padding=1),\n","            nn.LeakyReLU(0.1, inplace=True),\n","            nn.ConvTranspose2d(128, 3, kernel_size=5, stride=2, padding=2, output_padding=1),\n","            nn.Softmax()  # To get pixel values in the range [0, 1]\n","        )\n","\n","    def forward(self, x):\n","        encoded = self.encoder(x)\n","        return encoded\n","\n","    def decode(self, encoding):\n","        x = self.decoder_fc(encoding)\n","        x = self.decoder_conv(x)\n","        return x\n","\n","# Create model, loss function, and optimizer\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = Classifier().to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.0001)"]},{"cell_type":"code","source":["!pip install torchsummary\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ht3aDaqEHV37","executionInfo":{"status":"ok","timestamp":1748338834212,"user_tz":-330,"elapsed":3530,"user":{"displayName":"Mishab ch","userId":"00818002393088854647"}},"outputId":"2fdf7624-c8d7-41f3-f0d1-8460e7929a5c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchsummary in /usr/local/lib/python3.11/dist-packages (1.5.1)\n"]}]},{"cell_type":"code","source":["from torchsummary import summary\n","import torch\n","\n","# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Initialize your model and move it to the device\n","model = Classifier().to(device)\n","\n","# Display model architecture\n","# Change the input_size to reflect 1 input channel as expected by the model\n","summary(model, input_size=(1, 128, 128))\n","summary(model, output_size=(1, 256, 256))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":610},"id":"daL4mt9XHXPv","executionInfo":{"status":"error","timestamp":1748339062807,"user_tz":-330,"elapsed":3665,"user":{"displayName":"Mishab ch","userId":"00818002393088854647"}},"outputId":"a88ae374-636f-4060-a31c-7b7f5b2d6cb0"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1          [-1, 128, 64, 64]           3,328\n","         LeakyReLU-2          [-1, 128, 64, 64]               0\n","            Conv2d-3          [-1, 256, 32, 32]         819,456\n","         LeakyReLU-4          [-1, 256, 32, 32]               0\n","            Conv2d-5          [-1, 512, 16, 16]       3,277,312\n","         LeakyReLU-6          [-1, 512, 16, 16]               0\n","            Conv2d-7           [-1, 1024, 8, 8]      13,108,224\n","         LeakyReLU-8           [-1, 1024, 8, 8]               0\n","           Flatten-9                [-1, 65536]               0\n","           Linear-10                 [-1, 1024]      67,109,888\n","        LeakyReLU-11                 [-1, 1024]               0\n","           Linear-12                    [-1, 2]           2,050\n","================================================================\n","Total params: 84,320,258\n","Trainable params: 84,320,258\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.06\n","Forward/backward pass size (MB): 15.52\n","Params size (MB): 321.66\n","Estimated Total Size (MB): 337.23\n","----------------------------------------------------------------\n"]},{"output_type":"error","ename":"TypeError","evalue":"summary() got an unexpected keyword argument 'output_size'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-f821b16f28af>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Change the input_size to reflect 1 input channel as expected by the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: summary() got an unexpected keyword argument 'output_size'"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"bhfPkle2N7vj"},"outputs":[],"source":["def train(model, dataloader, criterion, optimizer):\n","    model.train()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","    fake_count = 0\n","    real_count = 0\n","\n","\n","    datal=tqdm.tqdm(dataloader)\n","    for images, labels in datal:\n","\n","        images, labels = images.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        _, predicted = outputs.max(1)\n","        total += labels.size(0)\n","        correct += predicted.eq(labels).sum().item()\n","\n","        # Counting real and fake images\n","        fake_count += (predicted == 1).sum().item()\n","        real_count += (predicted == 0).sum().item()\n","\n","        datal.set_postfix(loss=running_loss)\n","\n","    accuracy = 100. * correct / total\n","    outputs=torch.argmax(outputs, dim=1)\n","    prec=precision(outputs,labels)\n","    return running_loss / len(dataloader), accuracy, fake_count, real_count, prec"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lVtYgGHNR6Dm","scrolled":false},"outputs":[],"source":["n_epochs = 1\n","val_accuracies = []\n","for epoch in range(n_epochs):\n","    train_loss, train_acc, train_fake_count, train_real_count, prec = train(model, trainloader, criterion, optimizer)\n","    #val_loss, val_acc, val_fake_count, val_real_count, y_true, y_pred = validate(model, test_loader, criterion)\n","    #val_accuracies.append(val_acc)\n","    #print(f'Epoch {epoch+1}/{n_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n","    print(f'Epoch {epoch+1}/{n_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n","    print(f'Training: Fake Images: {train_fake_count}, Real Images: {train_real_count}')\n","    #print(f'Validation: Fake Images: {val_fake_count}, Real Images: {val_real_count}')\n","    log_to_csv('training_log.csv', [epoch, train_loss, train_acc, prec])\n","    #torch.save(model.state_dict(),'rf_face_det_weights.pth')\n","    #torch.save(optimizer.state_dict(),'rf_face_det_opt.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NiNeOYi2e71o"},"outputs":[],"source":["torch.save(model.state_dict(),'rf_face_det_weights.pth')\n","torch.save(optimizer.state_dict(),'rf_face_det_opt.pth')"]},{"cell_type":"markdown","metadata":{"id":"0UqyrYyYOfe9"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":0,"status":"ok","timestamp":1721985239644,"user":{"displayName":"Mishab ch","userId":"00818002393088854647"},"user_tz":-330},"id":"2Cbj380Qe71p","outputId":"d28b1699-f7eb-452b-fe13-692dfae847d7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting facenet-pytorch\n","  Downloading facenet_pytorch-2.6.0-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: numpy<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (1.25.2)\n","Collecting Pillow<10.3.0,>=10.2.0 (from facenet-pytorch)\n","  Downloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n","Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (2.31.0)\n","Collecting torch<2.3.0,>=2.2.0 (from facenet-pytorch)\n","  Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n","Collecting torchvision<0.18.0,>=0.17.0 (from facenet-pytorch)\n","  Downloading torchvision-0.17.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: tqdm<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (4.66.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2024.7.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n","  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Collecting triton==2.2.0 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n","  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<2.3.0,>=2.2.0->facenet-pytorch)\n","  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.3.0,>=2.2.0->facenet-pytorch) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.3.0,>=2.2.0->facenet-pytorch) (1.3.0)\n","Downloading facenet_pytorch-2.6.0-py3-none-any.whl (1.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hUsing cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchvision-0.17.2-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: triton, Pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, facenet-pytorch\n","  Attempting uninstall: triton\n","    Found existing installation: triton 2.3.1\n","    Uninstalling triton-2.3.1:\n","      Successfully uninstalled triton-2.3.1\n","  Attempting uninstall: Pillow\n","    Found existing installation: Pillow 9.4.0\n","    Uninstalling Pillow-9.4.0:\n","      Successfully uninstalled Pillow-9.4.0\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.3.1+cu121\n","    Uninstalling torch-2.3.1+cu121:\n","      Successfully uninstalled torch-2.3.1+cu121\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.18.1+cu121\n","    Uninstalling torchvision-0.18.1+cu121:\n","      Successfully uninstalled torchvision-0.18.1+cu121\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.2.0 which is incompatible.\n","torchaudio 2.3.1+cu121 requires torch==2.3.1, but you have torch 2.2.2 which is incompatible.\n","torchtext 0.18.0 requires torch>=2.3.0, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed Pillow-10.2.0 facenet-pytorch-2.6.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 torch-2.2.2 torchvision-0.17.2 triton-2.2.0\n"]},{"data":{"application/vnd.colab-display-data+json":{"id":"8b768fdadf414769a5230b2a8c82af55","pip_warning":{"packages":["PIL","torch","torchgen","torchvision"]}}},"metadata":{},"output_type":"display_data"}],"source":["pip install facenet-pytorch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ryROdkV96lqn"},"outputs":[],"source":["import csv\n","import os\n","\n","def log_to_csv(filename, data):\n","    file_exists = os.path.isfile(filename)\n","    with open(filename, mode='a', newline='') as csv_file:  # Use newline='' to avoid extra empty rows\n","        writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n","        if not file_exists:  # Write header if file is newly created\n","            writer.writerow([\"Epoch\", \"Train Loss\", \"Train Accuracy\", \"Precision\"])\n","        writer.writerow(data)  # Write the data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":66,"status":"error","timestamp":1722308676924,"user":{"displayName":"Mishab ch","userId":"00818002393088854647"},"user_tz":-330},"id":"mRg20Q8Qe71p","outputId":"3f42a16e-adbf-42b7-f569-a1cfb7db5bf1"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'facenet_pytorch'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-cac8b2084eab>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfacenet_pytorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMTCNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInceptionResnetV1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmtcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMTCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'facenet_pytorch'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}],"source":["from facenet_pytorch import MTCNN, InceptionResnetV1\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","mtcnn = MTCNN()\n","model = Classifier().to(device)\n","image_path = '/home/developer/Downloads/example3'\n","image = Image.open(image_path)\n","boxes, probs = mtcnn.detect(image)\n","def crop_faces(image, boxes):\n","    faces = []\n","    for box in boxes:\n","        x1, y1, x2, y2 = box.astype(int)\n","        faces.append(image.crop((x1, y1, x2, y2)))\n","    return faces\n","cropped_faces = crop_faces(image, boxes)\n","num_faces = len(cropped_faces)\n","if num_faces > 0:\n","    plt.imshow(cropped_faces[0])\n","    plt.axis(False)\n","    plt.show()\n","else:\n","    print(\"No faces detected.\")\n","\n","\n","y=data_define(tensor=cropped_faces[0].convert('RGB')).cuda()\n","outputs=model(y.unsqueeze(0))\n","pred=torch.argmax(outputs,axis=1)\n","if pred==0:\n","    print('real')\n","else:\n","    print('fake')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XALNrRcUe71p"},"outputs":[],"source":["import cv2\n","import torch\n","from facenet_pytorch import MTCNN\n","from torchvision import transforms\n","from PIL import Image\n","import os\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","mtcnn = MTCNN(keep_all=True, device=device)\n","\n","def load_model(a):\n","    model = a\n","    model.load_state_dict(torch.load('rf_face_det_weights.pth'))\n","    model.to(device)\n","    model.eval()\n","    return model\n","def crop_faces(image, boxes):\n","    faces = []\n","    for box in boxes:\n","        x1, y1, x2, y2 = box.astype(int)\n","        faces.append(image.crop((x1, y1, x2, y2)))\n","    return faces\n","video_path = '/home/developer/Celeb-real/id0_0005.mp4'\n","cap = cv2.VideoCapture(video_path)\n","if not cap.isOpened():\n","    print(f\"Error opening video file {video_path}\")\n","    exit()\n","while cap.isOpened():\n","    ret, frame = cap.read()\n","    if ret:\n","        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","        boxes, probs = mtcnn.detect(rgb_frame)\n","        if boxes is not None:\n","            for box in boxes:\n","                startX, startY, endX, endY = box.astype(int)\n","                face_region = frame[startY:endY, startX:endX]\n","                pil_image = Image.fromarray(cv2.cvtColor(face_region, cv2.COLOR_BGR2RGB))\n","                input_tensor = data_define(tensor=pil_image.convert('RGB')).cuda()\n","                outputs = model(input_tensor.unsqueeze(0))\n","                prediction = torch.argmax(outputs, dim=1).item()\n","                if prediction == 0:\n","                    label = \"Real\"\n","                    color = (0, 255, 0)\n","                else:\n","                    label = \"Fake\"\n","                    color = (0, 0, 255)\n","                cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n","                cv2.putText(frame, label, (startX, startY - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2, cv2.LINE_AA)\n","        cv2.imshow('Frame', frame)\n","        if cv2.waitKey(25) & 0xFF == ord('q'):\n","            break\n","    else:\n","        break\n","cap.release()\n","cv2.destroyAllWindows()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rAYz62_Ue71q"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ItdRWtdoe71q"},"outputs":[],"source":["import csv\n","def log_to_csv(filename, data):\n","    file_exists = os.path.isfile(filename)\n","    with open(filename, mode='a') as csv_file:\n","        with open(filename, 'r', newline='') as file:\n","            csv_reader = csv.reader(file)\n","            l=[row for row in csv_reader]\n","            if len(l)>0 and data[0]==0:\n","                if l[-1][0].isdigit():  # Ensure the last row's first element is a digit\n","                    data[0] = int(l[-1][0]) + data[0] + 1\n","                else:\n","                    data[0] = 1\n","        writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n","        if not file_exists:\n","            writer.writerow(['Epoch', 'Loss', 'Accuracy', 'Precision'])\n","        writer.writerow(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":456},"executionInfo":{"elapsed":1923,"status":"error","timestamp":1721933614593,"user":{"displayName":"Mishab ch","userId":"00818002393088854647"},"user_tz":-330},"id":"5Xp8RYMWe71q","outputId":"0ab96689-3031-4aa1-9364-cde1cf40f82d"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement train_mod (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for train_mod\u001b[0m\u001b[31m\n","\u001b[0m"]},{"ename":"ModuleNotFoundError","evalue":"No module named 'train_mod'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-41-008b4dc7db65>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# If 'train_mod' is a local file, make sure it's in the same directory as this script or in a directory included in your Python path.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrain_mod\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_define\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'train_mod'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}],"source":["!pip install train_mod # Assuming 'train_mod' is a valid package on PyPI. If it's a local module, adjust the path accordingly.\n","import torch\n","from torch import nn, optim\n","from torchvision.transforms import Compose\n","import csv\n","import os\n","# If 'train_mod' is a local file, make sure it's in the same directory as this script or in a directory included in your Python path.\n","from train_mod import Classifier, data_define, train\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","inp=int(input('Train/Inference')) #input must 0/1\n","if inp==0:\n","    ds, trainloader=data_define('/home/developer')\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model = Classifier().to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","    if os.path.exists('rf_face_det_weights.pth') and os.path.exists('rf_face_det_opt.pth'):\n","        model.load_state_dict(torch.load('rf_face_det_weights.pth'))\n","        optimizer.load_state_dict(torch.load('rf_face_det_opt.pth'))\n","    n_epochs = 30 #changeable parameter\n","    for epoch in range(n_epochs):\n","        train_loss, train_acc, train_fake_count, train_real_count, prec, cm = train(model, trainloader, criterion, optimizer)\n","\n","        print(f'Epoch {epoch+1}/{n_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n","        print(f'Training: Fake Images: {train_fake_count}, Real Images: {train_real_count}')\n","        print('Confusion Matrix')\n","        print(cm)\n","        log_to_csv('train.csv', [epoch, train_loss, train_acc, prec])\n","        torch.save(model.state_dict(),'rf_face_det_weights.pth')\n","        torch.save(optimizer.state_dict(),'rf_face_det_opt.pth')\n","elif inp==1:\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model = Classifier().to(device)\n","    model.load_state_dict(torch.load('rf_face_det_weights.pth'))\n","    model.to(device)\n","    st=int(input('Image/Video'))\n","    if st==0:\n","        from facenet_pytorch import MTCNN, InceptionResnetV1\n","        from PIL import Image\n","        import matplotlib.pyplot as plt\n","        mtcnn = MTCNN()\n","        model = Classifier().to(device)\n","        image_path = '/home/developer/Downloads/example3'\n","        image = Image.open(image_path)\n","        boxes, probs = mtcnn.detect(image)\n","        def crop_faces(image, boxes):\n","            faces = []\n","            for box in boxes:\n","                x1, y1, x2, y2 = box.astype(int)\n","                faces.append(image.crop((x1, y1, x2, y2)))\n","            return faces\n","        cropped_faces = crop_faces(image, boxes)\n","        num_faces = len(cropped_faces)\n","        if num_faces > 0:\n","            plt.imshow(cropped_faces[0])\n","            plt.axis(False)\n","            plt.show()\n","        else:\n","            print(\"No faces detected.\")\n","\n","\n","        y=data_define(tensor=cropped_faces[0].convert('RGB')).cuda()\n","        outputs=model(y.unsqueeze(0))\n","        pred=torch.argmax(outputs,axis=1)\n","        if pred==0:\n","            print('real')\n","        else:\n","            print('fake')\n","    elif st==1:\n","        import cv2\n","        import torch\n","        from facenet_pytorch import MTCNN\n","        from torchvision import transforms\n","        from PIL import Image\n","        import os\n","        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        mtcnn = MTCNN(keep_all=True, device=device)\n","\n","        def load_model(a):\n","            model = a\n","            model.load_state_dict(torch.load('rf_face_det_weights.pth'))\n","            model.to(device)\n","            model.eval()\n","            return model\n","        def crop_faces(image, boxes):\n","            faces = []\n","            for box in boxes:\n","                x1, y1, x2, y2 = box.astype(int)\n","                faces.append(image.crop((x1, y1, x2, y2)))\n","            return faces\n","        video_path = '/home/developer/Celeb-real/id0_0005.mp4'\n","        cap = cv2.VideoCapture(video_path)\n","        if not cap.isOpened():\n","            print(f\"Error opening video file {video_path}\")\n","            exit()\n","        while cap.isOpened():\n","            ret, frame = cap.read()\n","            if ret:\n","                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","                boxes, probs = mtcnn.detect(rgb_frame)\n","                if boxes is not None:\n","                    for box in boxes:\n","                        startX, startY, endX, endY = box.astype(int)\n","                        face_region = frame[startY:endY, startX:endX]\n","                        pil_image = Image.fromarray(cv2.cvtColor(face_region, cv2.COLOR_BGR2RGB))\n","                        input_tensor = data_define(tensor=pil_image.convert('RGB')).cuda()\n","                        outputs = model(input_tensor.unsqueeze(0))\n","                        prediction = torch.argmax(outputs, dim=1).item()\n","                        if prediction == 0:\n","                            label = \"Real\"\n","                            color = (0, 255, 0)\n","                        else:\n","                            label = \"Fake\"\n","                            color = (0, 0, 255)\n","                        cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n","                        cv2.putText(frame, label, (startX, startY - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2, cv2.LINE_AA)\n","                cv2.imshow('Frame', frame)\n","                if cv2.waitKey(25) & 0xFF == ord('q'):\n","                    break\n","            else:\n","                break\n","        cap.release()\n","        cv2.destroyAllWindows()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BeqppfOFe71r"},"outputs":[],"source":["def confusion_matrix:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pP_XHyPve71r"},"outputs":[],"source":["\n","actual = np.random.randn(100)\n","predicted = np.random.randn(100)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ChjmggqSe71r"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}